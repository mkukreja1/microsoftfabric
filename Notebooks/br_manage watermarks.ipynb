{"cells":[{"cell_type":"code","source":["from delta.tables import *\n","# import datetime \n","import time\n","\n","# dt = str(datetime.date.today())\n","year, month, day = time.strftime(\"%Y\"), time.strftime(\"%m\"), time.strftime(\"%d\")\n","hour, minute, second = time.strftime(\"%H\"), time.strftime(\"%M\"), time.strftime(\"%S\")\n","\n","columns = [\"db_name\", \"schema_name\", \"table_name\", \"incremental_column\", \"incremental_column_value\", \"merge_key_column\", \"delta_source_path\"]\n","values = [(\"lakehouse\", \"dbo\", \"products\", \"updated_at\", \"1900-01-01 00:00:00.000\", \"product_code\",\"\"),\n","       (\"lakehouse\", \"dbo\", \"store_customers\", \"updated_at\", \"1900-01-01 00:00:00.000\", \"customer_id\",\"\"),\n","       (\"lakehouse\", \"dbo\", \"store_orders\", \"updated_at\", \"1900-01-01 00:00:00.000\", \"orders_number\",\"\"),\n","\t   (\"lakehouse\", \"dbo\", \"inventory\", \"updated_at\", \"1900-01-01 00:00:00.000\", \"product\",\"\")]\n","\n","if not spark.catalog.tableExists(\"bronze_control_table\"):\n","    DeltaTable.createIfNotExists(spark) \\\n","        .tableName(\"bronze_control_table\") \\\n","        .addColumn(\"db_name\", \"STRING\") \\\n","        .addColumn(\"schema_name\", \"STRING\") \\\n","        .addColumn(\"table_name\", \"STRING\") \\\n","        .addColumn(\"incremental_column\", \"STRING\") \\\n","        .addColumn(\"incremental_column_value\", \"TIMESTAMP\", comment = \"updated on each run\") \\\n","        .addColumn(\"merge_key_column\", \"STRING\") \\\n","        .addColumn(\"delta_source_path\", \"STRING\", comment = \"updated on each run\") \\\n","        .execute()\n","        \n","    df = spark.createDataFrame(values, columns)\n","    df = df.withColumn(\"incremental_column_value\",df.incremental_column_value.cast(\"timestamp\"))\n","    df.write.mode(\"append\").format(\"delta\").saveAsTable(\"bronze_control_table\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"e61256f5-6ff5-416d-8911-d3957737116e"},{"cell_type":"code","source":["def get_tables_list():\n","    _df = spark.read.table(\"bronze_control_table\")\n","    display(_df)\n","    tbls = _df.select(\"table_name\").collect()\n","\n","    return tbls\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"85fb2278-2907-4ebb-9424-10ddfe5f3c2b"},{"cell_type":"code","source":["def create_and_update_delta_source_path(_table, _year, _month, _day, _hour, _minute):\n","    path = f\"abfss://Training@onelake.dfs.fabric.microsoft.com/trg_lakehouse.Lakehouse/Files/bronze/{_table}/processed/{_year}/{_month}/{_day}/{_hour}/{_minute}\"\n","   \n","    try:\n","        if not mssparkutils.fs.exists(path):\n","            print(f\"path does not exit, creating path: {path}\")\n","            mssparkutils.fs.mkdirs(path)\n","        else:\n","            print(f\"path: {path} already exists\")\n","    except Exception as e:\n","        print(e)\n","\n","    # folder = f\"abfss://Training@onelake.dfs.fabric.microsoft.com/trg_lakehouse.Lakehouse/Files/bronze/{_table}/processed/{_year}/{_month}/{_day}/{_hour}/{_minute}\"\n","    folder = f\"bronze/{_table}/processed/{_year}/{_month}/{_day}/{_hour}/{_minute}\"\n","   \n","    query = f\"UPDATE bronze_control_table SET delta_source_path = '{folder}' WHERE table_name = '{_table}'\"\n","    spark.sql(query)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"1c7a54e2-9ebf-47ab-8198-9ebe5d8c40bc"},{"cell_type":"code","source":["tables_to_process = get_tables_list()\n","for row in tables_to_process:\n","    table = row[\"table_name\"]\n","    create_and_update_delta_source_path(table, year, month, day, hour, minute)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"89c1a120-2ad1-42c9-8a7c-50abb6d5ed63"},{"cell_type":"code","source":["%%sql\n","\n","SELECT table_name, delta_source_path FROM bronze_control_table\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql"}},"id":"c07ab3f9-e764-4143-84d4-936a9d9ba65f"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"cec0f926-b748-4ee3-a0e7-af9b500519a4","known_lakehouses":[{"id":"cec0f926-b748-4ee3-a0e7-af9b500519a4"}],"default_lakehouse_name":"trg_lakehouse","default_lakehouse_workspace_id":"38931ff9-574b-4a30-a289-ce181a0c7917"}}},"nbformat":4,"nbformat_minor":5}